logging_params: # Logging for Weights and biases
  log_to_screen: !!bool True # Control detailed print statements for debugging/monitoring
  diagnostic_logs: !!bool False # Log Diagnistic  to screen
  log_to_wandb: !!bool False # True False
  wandb_table_logging_interval: 10 # not setup
  wandb_project: 'Model_Collapse_last_gen'
  wandb_group: 'static'
  wandb_name: 'UDM_s3_Oaw_lrC3e4'

dataset_params:
  # the data is contained in the 'data' folder in the root directory. there should be another 'mean_std.npz' file in the same folder with mean ans std values
  # data_dir: 'data_mnist/train/images'
  data_dir: '../data/Re500_fkx4fky4_r0.1_b20/NoSGS/NX64/dt0.0005_IC1/'
  
  file_range: [10000, 17500] # range of files to load
  # total samples used for training = file_range[1] - file_range[0] // step_size
  step_size: 3 # step size for data loading (every nth sample)
  condition_step_size: 1 # step size for conditioning data loading (every nth sample)
  num_prev_conditioning_steps : 1 # 1 2 3 4 # Number of previous time steps to use for conditioning t-1, t-2, t-3, t-4
  downsample_factor: 1
  downsample_procedure: 'spectral' # spectral physical
  normalize: !!bool True

diffusion_params:
  conditional: !!bool True # True False
  condition_noise: !!bool False  # True False Noise is added to the conditioning while training and sampling ** Not implemented
  num_timesteps : 1000
  beta_start : 0.0001
  beta_end : 0.02

model_params: # U-NET
  padding_mode: 'circular' # Padding of conv2d layers. 'zeros': standard non-periodic UNET; 'circular': periodic UNET (maybe need coord_conv=True for UDM )
  im_channels : 2 # 2 when conditional=False
  pred_channels: 2 # =im_channels for conditional=False; 
  cond_channels: 2 # =2*(num_prev_conditioning_steps); 2,4,6... when conditional=True;Will be added to input channels if conditional=True; = 0 for conditional=False
  coord_conv: !!bool False # True False # Concatenate coordinates with input channels; True: Add +2 channels to input_channels
  # input_channels = im_channels + cond_channels (if conditional) + 2 (if coord_conv)
  # Note: A periodic UNET is translational invariant leading to translational invariant outputs; We need to feed positional argumenets to produce postional aware outputs (coord_conv=True)
  im_size : 64
  down_channels : [32, 64, 128, 256]
  mid_channels : [256, 256, 128]
  down_sample : [True, True, False]
  time_emb_dim : 128
  num_down_layers : 2
  num_mid_layers : 2
  num_up_layers : 2
  num_heads : 4

train_params:
  task_name: 'CDM_test'
  effective_batch_size: 16 # batch_size per gpu = effective_batch_size / num_gpus
  num_epochs: 800
  optimizer: 'adamw' # 'adam' 'adamw'
  scheduler: CosineAnnealingLR # 'CosineAnnealingLR' 'CosineAnnealingWarmRestarts' 'ReduceLROnPlateau'
  lr: 1e-4
  lr_min: 1e-6 # Minimum learning rate # CosineAnnealingLR CosineAnnealingWarmRestarts ReduceLROnPlateau
  # CosineAnnealingWarmRestarts
  T_0: 200 # Number of epochs until the first restart # 
  T_mult: 1 # Multiplicative factor for the number of iterations between restarts # CosineAnnealingWarmRestarts
  # ReduceLROnPlateau
  factor : 0.5 # factor for reducing learning rate
  patience : 20 # patience for reducing learning rate
  cooldown : 10 # cooldown for reducing learning rate

  warmup: !!bool False # Warmup for LR scheduler (Start with low LR and then increase)
  warmup_start_factor: 0.001 
  warmup_total_iters: 5 # Warm-up learning rate for the first N epochs

  weight_decay: 1e-6 # L2 regularization Loss
  clip_grad_norm: null # 1.0 # Gradient clipping to avoid exploding gradients, null for no clipping
  loss: 'noise' # sample noise 
  global_seed: 1 # Global seed for training reproducibility

  divergence_loss: !!bool False # True False
  divergence_loss_type: 'denoise_sample' # denoise_sample (need loss = noise), direct_sample (loss == noise or sample)
  divergence_loss_weight: 1

  # Following only used for divergence_loss_type: denoise_sample
  denoise_sample_timestep: 1 # 1 - num_timesteps  
  denoise_sample_batch_ratio: 0.15 # % of batch used for divergence loss, rest of the samples used for mse loss

  model_collapse: !!bool False # True False # Not implemented for conditional
  model_collapse_type: 'last_gen' # all_gen last_gen # All generations vs last generation
  model_collapse_gen: 1 # 1 2 3 4 5 # Can't be gen 0 - model_collapse should be False for gen 0
  # Only used for emulator data when conditional=True
  ensemble_name: '1_0' #  all: analyze data in all folder, <ensemble_name>: It will only analyze the data of single ensemble in a particular folder.
  file_range: [0, 99] # range of files to load

  ckpt_name: 'ddpm_ckpt.pth'
  best_ckpt_name: 'best_ddpm_ckpt.pth'

# For Sampling (Inference)
sample_params:
  global_seed: null # Global seed for reproducibility; If null, it will be random seed - It should be null to produce different samples for each ensemble

  sampler: 'ddpm'   # ddpm ddim dpmsolver dpmsolver++ # Sampling method
  dpm_steps: 1000           # ↓ = faster, ↑ = better quality
  dpm_order: 1               # '1' '2' '3'  (3 recommended for unconditional; 1: equivalent to DDIM)
  dpm_method: "singlestep"  # 'singlestep' 'multistep' 'adaptive'
  dpm_skip: "time_uniform"  # 'time_uniform' 'logSNR'
  dpm_guidance: 'uncond'    # 'uncond' # only one comaptible with current code

  sample_batch_size : 1        # Inference data batch size
  num_sample_batch :  100       # number of samples = sample_batch_size * num_sample_batch
  # sample_file_start_idx : 5000  # For conditional:True, first real frame used to seed sampling 
  sample_start_idx_file_range: [5000, 5100] # the initial condition for sampling will be randomly taken from this range
  save_image: !!bool True       # Saves snapshots of emulated data
  save_data: !!bool True # Saved emulated data # First five batches will always be saved