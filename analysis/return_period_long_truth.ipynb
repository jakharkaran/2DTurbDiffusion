{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.io import loadmat\n",
    "from scipy import stats\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from return_period import percentile_data, return_period, return_period_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Truth\n",
    "\n",
    "# Load the data\n",
    "\n",
    "DATA_DIR = '/ocean/projects/phy220045p/jakhar/py2d_dealias/postprocess/results_aposteriori/Re500_fkx4fky4_r0.1_b20/'\n",
    "filename = 'omega_extreme_DNS_NX64_dt0.0005_IC1'\n",
    "dt_data = 2e-2\n",
    "\n",
    "\n",
    "# ## Ground Truth\n",
    "\n",
    "Omega_max_arr = []\n",
    "Omega_min_arr = []\n",
    "\n",
    "for ic in range(1,11):\n",
    "    filename = f'omega_extreme_DNS_NX64_dt0.0005_IC{ic}.mat'\n",
    "    data = loadmat(DATA_DIR + '/' + filename)\n",
    "\n",
    "    Omega_max_arr.append(data['Omega_max'])\n",
    "    Omega_min_arr.append(data['Omega_min'])\n",
    "\n",
    "    omega_mean = data['Omega_mean']\n",
    "    omega_std = data['Omega_std']\n",
    "\n",
    "    print(data['Omega_mean'], data['Omega_std'], np.asarray(data['Omega_max']).shape, np.asarray(data['Omega_min']).shape)\n",
    "\n",
    "omega_max = np.asarray(Omega_max_arr).flatten()\n",
    "omega_min = np.asarray(Omega_min_arr).flatten()\n",
    "\n",
    "omega_max_normalized = omega_max / omega_std[0][0]\n",
    "omega_min_normalized = omega_min / omega_std[0][0]\n",
    "\n",
    "# desired_length = 25000000\n",
    "# while len(omega_max_normalized) > desired_length:\n",
    "#     omega_max_normalized = omega_max_normalized[:desired_length]\n",
    "#     omega_min_normalized = omega_min_normalized[:desired_length]\n",
    "\n",
    "split_size = 100\n",
    "omega_max_normalized_split = np.split(omega_max_normalized, split_size)\n",
    "omega_min_normalized_split = np.split(omega_min_normalized, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_data(data, percentile):\n",
    "    \"\"\"\n",
    "    Calculate error bands and\n",
    "    return the lower/upper bounds in percentile\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        2D NumPy array of shape (N, samples)\n",
    "    percentile : float\n",
    "        A number between 0 and 100 (typically <= 50 for symmetrical bounds)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    means : np.ndarray\n",
    "        Array of sample means (shape: N)\n",
    "    lower_bounds: np.ndarray\n",
    "        Array of lower bounds in percentage relative to the mean (shape: N)\n",
    "    upper_bounds: np.ndarray\n",
    "        Array of upper bounds in percentage relative to the mean (shape: N)\n",
    "    \"\"\"\n",
    "    # Mean of each row\n",
    "    means = np.mean(data, axis=0)\n",
    "    \n",
    "    # Lower (p-th) and upper ((100-p)-th) percentiles of each row\n",
    "    lower_vals = np.percentile(data, percentile, axis=0)\n",
    "    upper_vals = np.percentile(data, 100 - percentile, axis=0)\n",
    "\n",
    "    print(data.shape)\n",
    "    # print(lower_vals.shape, upper_vals.shape)\n",
    "    # print(lower_vals, upper_vals)\n",
    "    \n",
    "    # Calculate percentage difference relative to the mean\n",
    "    # (m - L)/m * 100 for lower, (U - m)/m * 100 for upper\n",
    "    # lower_bounds = 100.0 * (means - lower_vals) / means\n",
    "    # upper_bounds = 100.0 * (upper_vals - means) / means\n",
    "    \n",
    "    return means, lower_vals, upper_vals\n",
    "\n",
    "def std_dev_data(data, std_dev=1):\n",
    "    \"\"\"\n",
    "    Calculate error bands and\n",
    "    return the lower/upper bounds in percentile\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        2D NumPy array of shape (N, samples)\n",
    "    std_dev : float\n",
    "        A number between 0 and 100 (typically <= 50 for symmetrical bounds)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    means : np.ndarray\n",
    "        Array of sample means (shape: N)\n",
    "    lower_bounds: np.ndarray\n",
    "        Array of lower bounds in percentage relative to the mean (shape: N)\n",
    "    upper_bounds: np.ndarray\n",
    "        Array of upper bounds in percentage relative to the mean (shape: N)\n",
    "    \"\"\"\n",
    "    # Mean of each row\n",
    "    means = np.mean(data, axis=0)\n",
    "    stds = np.std(data, axis=0)\n",
    "    \n",
    "    # Lower (p-th) and upper ((100-p)-th) percentiles of each row\n",
    "    lower_vals = means - stds*std_dev\n",
    "    upper_vals = means + stds*std_dev\n",
    "    \n",
    "    return means, lower_vals, upper_vals\n",
    "\n",
    "def return_period_ensemble(data, dt=1, bins=50, uncertainty='freq_exceedance',\n",
    "                           confidence_level=25):\n",
    "    '''\n",
    "    Calculate return period using ensemble of data\n",
    "    data: 2D array of data [ensemble, time]\n",
    "    dt: time step\n",
    "    bins: number of bins for binning the data\n",
    "    uncertainty: 'freq' / 'freq_exceedance': \n",
    "        Determines whether uncertainty is calculated based on frequency or frequency exceedance\n",
    "    confidence_level: Percentage for the confidence interval (e.g., 95 for 95% confidence interval)\n",
    "    '''\n",
    "    number_ensemble = data.shape[0]\n",
    "    total_data_points = data.shape[1]\n",
    "    \n",
    "    bin_min = np.min(data)\n",
    "    bin_max = np.max(data)\n",
    "    \n",
    "    # Initialize lists to store per-ensemble probabilities\n",
    "    prob_exceedance_ensemble = []\n",
    "    freq_ensemble = []\n",
    "\n",
    "    bins_centers = None  # Will be set during the first iteration\n",
    "\n",
    "    for i in range(number_ensemble):\n",
    "        data_ensemble = data[i, :]\n",
    "        freq_sample, bin_edges = np.histogram(data_ensemble, bins=bins, range=(bin_min, bin_max))\n",
    "        if bins_centers is None:\n",
    "            bins_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "        freq_exceedance_sample = np.cumsum(freq_sample[::-1])[::-1]\n",
    "        prob_exceedance_sample = freq_exceedance_sample / total_data_points\n",
    "\n",
    "        freq_ensemble.append(freq_sample)\n",
    "        prob_exceedance_ensemble.append(prob_exceedance_sample)\n",
    "\n",
    "    freq_ensemble = np.array(freq_ensemble)\n",
    "    prob_exceedance_ensemble = np.array(prob_exceedance_ensemble)\n",
    "\n",
    "    if uncertainty == 'freq' or uncertainty == 'freq_exceedance':\n",
    "        # Calculate uncertainty based on the selected method\n",
    "        if uncertainty == 'freq':\n",
    "            # Average frequencies and compute probabilities from the mean frequencies\n",
    "            freq_mean = np.mean(freq_ensemble, axis=0)\n",
    "\n",
    "            # Compute cumulative frequencies\n",
    "            freq_exceedance_mean = np.cumsum(freq_mean[::-1])[::-1]\n",
    "\n",
    "            # Calculate exceedance probabilities\n",
    "            prob_exceedance_mean = freq_exceedance_mean / (total_data_points * number_ensemble)\n",
    "\n",
    "            # Calculate confidence intervals for frequencies\n",
    "            lower_percentile = (100 - confidence_level) / 2\n",
    "            upper_percentile = 100 - lower_percentile\n",
    "\n",
    "            freq_exceedance_lower = np.percentile(np.cumsum(freq_ensemble[:, ::-1], axis=1)[:, ::-1],\n",
    "                                                lower_percentile, axis=0)\n",
    "            freq_exceedance_upper = np.percentile(np.cumsum(freq_ensemble[:, ::-1], axis=1)[:, ::-1],\n",
    "                                                upper_percentile, axis=0)\n",
    "\n",
    "            # Calculate exceedance probabilities for confidence intervals\n",
    "            prob_exceedance_lower = freq_exceedance_lower / (total_data_points)\n",
    "            prob_exceedance_upper = freq_exceedance_upper / (total_data_points)\n",
    "\n",
    "        elif uncertainty == 'freq_exceedance':\n",
    "            # Calculate mean of probabilities directly\n",
    "            # prob_exceedance_mean, prob_exceedance_lower, prob_exceedance_upper = ci_t_distribution(prob_exceedance_ensemble, confidence_level)\n",
    "            # prob_exceedance_mean, prob_exceedance_lower, prob_exceedance_upper = ci_normal_distribution(prob_exceedance_ensemble, confidence_level)\n",
    "            # prob_exceedance_mean, prob_exceedance_lower, prob_exceedance_upper = percentile_data(prob_exceedance_ensemble, confidence_level)\n",
    "            prob_exceedance_mean, prob_exceedance_lower, prob_exceedance_upper = std_dev_data(prob_exceedance_ensemble, 1)\n",
    "\n",
    "    \n",
    "        # print(prob_exceedance_mean)\n",
    "        # print(prob_exceedance_lower)\n",
    "        # print(prob_exceedance_upper)\n",
    "\n",
    "        # Ensure probabilities are within valid range\n",
    "        x = np.clip(prob_exceedance_mean, 1e-14, 1)\n",
    "        prob_exceedance_lower = np.clip(prob_exceedance_lower, 1e-14, 1)\n",
    "        prob_exceedance_upper = np.clip(prob_exceedance_upper, 1e-14, 1)\n",
    "\n",
    "        # Calculate return periods\n",
    "        return_period_mean = dt / prob_exceedance_mean\n",
    "        return_period_lower = dt / prob_exceedance_upper  # Higher probability, lower return period\n",
    "        return_period_upper = dt / prob_exceedance_lower  # Lower probability, higher return period\n",
    "\n",
    "    elif uncertainty == 'return_period':\n",
    "\n",
    "        prob_exceedance_ensemble = np.clip(prob_exceedance_ensemble, 1e-14, 1)\n",
    "        return_period_ensemble = dt/prob_exceedance_ensemble\n",
    "\n",
    "        return_period_mean, return_period_lower, return_period_upper = percentile_data(return_period_ensemble, confidence_level)\n",
    "        # return_period_mean, return_period_lower, return_period_upper = std_dev_data(return_period_ensemble, 1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid uncertainty method. Choose 'freq' or 'freq_exceedance'.\")\n",
    "\n",
    "    return return_period_mean, return_period_lower, return_period_upper, bins_centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "start=5000\n",
    "omega_max_train_normalized = omega_max_normalized[start:start+2500] \n",
    "omega_min_train_normalized = omega_min_normalized[start:start+2500] \n",
    "\n",
    "markersize = 1\n",
    "bin_num = 100\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "### Omega min\n",
    "\n",
    "return_period_kerry, freq_tot, bins_min = return_period(np.abs(omega_min_train_normalized), dt=dt_data, bins=bin_num)\n",
    "axes[0].semilogy(-bins_min, return_period_kerry, 'or', label='Train (2500 snapshots)', markersize=markersize)\n",
    "\n",
    "return_period_mean, return_period_min, return_period_max, bins = return_period_ensemble(\n",
    "    np.asarray(omega_max_normalized_split), dt=dt_data, bins=bin_num, uncertainty='return_period')\n",
    "axes[0].semilogy(-bins, return_period_mean, 'ok', label='Truth: 250,000 snapshots; 100 ensembles; 25/75 percentile', markersize=markersize)\n",
    "axes[0].fill_between(-bins, return_period_min, return_period_max, color='k', alpha=0.2)\n",
    "\n",
    "axes[0].set_xlabel('$\\omega$ min')\n",
    "axes[0].set_ylabel('Return Period')\n",
    "\n",
    "# ### Omega max\n",
    "\n",
    "return_period_kerry, freq_tot, bins_max = return_period(np.abs(omega_max_train_normalized), dt=dt_data, bins=bin_num)\n",
    "axes[1].semilogy(bins_max, return_period_kerry, 'or', label='Train (2500 snapshots)', markersize=markersize)\n",
    "\n",
    "return_period_mean, return_period_min, return_period_max, bins = return_period_ensemble(\n",
    "    np.asarray(omega_max_normalized_split), dt=dt_data, bins=bin_num, uncertainty='return_period')\n",
    "axes[1].semilogy(bins, return_period_mean, 'ok', label='Truth: 25/75 percentile; 100 ensembles; 250,000 snapshots', markersize=markersize)\n",
    "axes[1].fill_between(bins, return_period_min, return_period_max, color='k', alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel('$\\omega$ max')\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_ylim([1e-2, 1e3])\n",
    "axes[0].set_xlim([-4.5, -1.7])\n",
    "axes[1].set_xlim([1.7, 4.5])\n",
    "# plt.suptitle('97.5% CI error calculated with 200 ensembles of 2500 snapshots')\n",
    "axes[1].legend(loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "start=5000\n",
    "omega_max_train_normalized = omega_max_normalized[start:start+2500] \n",
    "omega_min_train_normalized = omega_min_normalized[start:start+2500] \n",
    "\n",
    "markersize = 1\n",
    "bin_num = 50\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "### Omega min\n",
    "\n",
    "return_period_kerry, freq_tot, bins_min = return_period(np.abs(omega_min_train_normalized), dt=dt_data, bins=bin_num)\n",
    "axes[0].semilogy(-bins_min, return_period_kerry, 'or', label='Train (2500 snapshots)', markersize=markersize)\n",
    "\n",
    "return_period_mean, return_period_min, return_period_max, bins = return_period_ensemble(\n",
    "    np.asarray(omega_max_normalized_split), dt=dt_data, bins=bin_num, uncertainty='freq_exceedance')\n",
    "axes[0].semilogy(-bins, return_period_mean, 'ok', label='Truth: 25/75 percentile; 100 ensembles; 250,000 snapshots', markersize=markersize)\n",
    "axes[0].fill_between(-bins, return_period_min, return_period_max, color='k', alpha=0.2)\n",
    "\n",
    "axes[0].set_xlabel('$\\omega$ min')\n",
    "axes[0].set_ylabel('Return Period')\n",
    "\n",
    "# ### Omega max\n",
    "\n",
    "return_period_kerry, freq_tot, bins_max = return_period(np.abs(omega_max_train_normalized), dt=dt_data, bins=bin_num)\n",
    "axes[1].semilogy(bins_max, return_period_kerry, 'or', label='Train (2500 snapshots)', markersize=markersize)\n",
    "\n",
    "return_period_mean, return_period_min, return_period_max, bins = return_period_ensemble(\n",
    "    np.asarray(omega_max_normalized_split), dt=dt_data, bins=bin_num, uncertainty='freq_exceedance')\n",
    "axes[1].semilogy(bins, return_period_mean, 'ok', label='Truth: 25/75 percentile; 100 ensembles; 250,000 snapshots', markersize=markersize)\n",
    "axes[1].fill_between(bins, return_period_min, return_period_max, color='k', alpha=0.2)\n",
    "\n",
    "axes[1].set_xlabel('$\\omega$ max')\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_ylim([1e-2, 1e6])\n",
    "# axes[0].set_xlim([-3, -2])\n",
    "axes[1].set_xlim([1.7, 5])\n",
    "plt.suptitle('97.5% CI error calculated with 200 ensembles of 2500 snapshots')\n",
    "axes[1].legend(loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 50\n",
    "# for count in range(omega_max_normalized_split.shape[0]):\n",
    "for count in range(len(omega_max_normalized_split)):\n",
    "    return_period_kerry, freq_tot, bins_max = return_period(np.abs(omega_max_normalized_split[count]), dt=dt_data, bins=bin_num)\n",
    "    plt.semilogy(bins_max, return_period_kerry, alpha = 0.1)\n",
    "\n",
    "return_period_mean, return_period_min, return_period_max, bins = return_period_ensemble(\n",
    "    np.asarray(omega_max_normalized_split), dt=dt_data, bins=bin_num, uncertainty='freq_exceedance')\n",
    "plt.semilogy(bins_max, return_period_kerry, 'k')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 50\n",
    "# for count in range(omega_max_normalized_split.shape[0]):\n",
    "for count in range(len(omega_max_normalized_split)):\n",
    "    return_period_kerry, freq_tot, bins_max = return_period(np.abs(omega_max_normalized_split[count]), dt=dt_data, bins=bin_num)\n",
    "    plt.semilogy(bins_max, return_period_kerry, alpha = 0.1)\n",
    "\n",
    "return_period_mean, return_period_min, return_period_max, bins = return_period_ensemble(\n",
    "    np.asarray(omega_max_normalized_split), dt=dt_data, bins=bin_num, uncertainty='freq_exceedance')\n",
    "plt.semilogy(bins_max, return_period_kerry, 'k')\n",
    "\n",
    "for count in range(100):\n",
    "    start = np.random.randint(5000,25025000)\n",
    "    omega_max_train_normalized = omega_max_normalized[start:start+25000] \n",
    "    return_period_kerry, freq_tot, bins_max = return_period(np.abs(omega_max_train_normalized), dt=dt_data, bins=bin_num)\n",
    "    plt.semilogy(bins_max, return_period_kerry, color='k', alpha = 0.2)\n",
    "\n",
    "\n",
    "for count in range(100):\n",
    "    start = np.random.randint(5000,25005000)\n",
    "    omega_max_train_normalized = omega_max_normalized[start:start+2500] \n",
    "    return_period_kerry, freq_tot, bins_max = return_period(np.abs(omega_max_train_normalized), dt=dt_data, bins=bin_num)\n",
    "    plt.semilogy(bins_max, return_period_kerry, color='r', alpha = 0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "dt = 1\n",
    "bins = 50\n",
    "confidence_level = 25\n",
    "uncertainity = 'return_period'\n",
    "\n",
    "data = np.random.rand(10, 500)\n",
    "\n",
    "number_ensemble = data.shape[0]\n",
    "total_data_points = data.shape[1]\n",
    "\n",
    "bin_min = np.min(data)\n",
    "bin_max = np.max(data)\n",
    "\n",
    "# Initialize lists to store per-ensemble probabilities\n",
    "prob_exceedance_ensemble = []\n",
    "freq_ensemble = []\n",
    "\n",
    "bins_centers = None  # Will be set during the first iteration\n",
    "\n",
    "for i in range(number_ensemble):\n",
    "    data_ensemble = data[i, :]\n",
    "    freq_sample, bin_edges = np.histogram(data_ensemble, bins=bins, range=(bin_min, bin_max))\n",
    "    if bins_centers is None:\n",
    "        bins_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "    freq_exceedance_sample = np.cumsum(freq_sample[::-1])[::-1]\n",
    "    prob_exceedance_sample = freq_exceedance_sample / total_data_points\n",
    "\n",
    "    freq_ensemble.append(freq_sample)\n",
    "    prob_exceedance_ensemble.append(prob_exceedance_sample)\n",
    "\n",
    "    print(prob_exceedance_sample.shape)\n",
    "\n",
    "freq_ensemble = np.array(freq_ensemble)\n",
    "prob_exceedance_ensemble = np.array(prob_exceedance_ensemble)\n",
    "\n",
    "prob_exceedance_ensemble = np.clip(prob_exceedance_ensemble, 1e-14, 1)\n",
    "\n",
    "return_period_ensemble = dt/prob_exceedance_ensemble\n",
    "prob_exceedance_mean, prob_exceedance_lower, prob_exceedance_upper = std_dev_data(prob_exceedance_ensemble, 1)\n",
    "\n",
    "            # prob_exceedance_mean, prob_exceedance_lower, prob_exceedance_upper = std_dev_data(prob_exceedance_ensemble, 1)\n",
    "\n",
    "    \n",
    "        # print(prob_exceedance_mean)\n",
    "        # print(prob_exceedance_lower)\n",
    "        # print(prob_exceedance_upper)\n",
    "\n",
    "        # # Ensure probabilities are within valid range\n",
    "        # x = np.clip(prob_exceedance_mean, 1e-14, 1)\n",
    "        # prob_exceedance_lower = np.clip(prob_exceedance_lower, 1e-14, 1)\n",
    "        # prob_exceedance_upper = np.clip(prob_exceedance_upper, 1e-14, 1)\n",
    "\n",
    "        # # Calculate return periods\n",
    "        # return_period_mean = dt / prob_exceedance_mean\n",
    "        # return_period_lower = dt / prob_exceedance_upper  # Higher probability, lower return period\n",
    "        # return_period_upper = dt / prob_exceedance_lower  # Lower probability, higher return period\n",
    "\n",
    "        # return return_period_mean, return_period_lower, return_period_upper, bins_centers\n",
    "\n",
    "\n",
    "\n",
    "# return_period_ensemble = np.zeros((number_ensemble, bins))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
